# TrainingArguments
seed: 42
max_steps: 6000  # will be overridden by sum(steps_per_stage)
report_to: wandb
run_name: smollm1-135m-2norm_r_4-progressive
save_strategy: steps
save_steps: 0.1
output_dir: ckpts/smollm1-135m-2norm_r_4-progressive
overwrite_output_dir: true
logging_strategy: steps
logging_steps: 1
resume_from_checkpoint: null
per_device_train_batch_size: 4
remove_unused_columns: False
gradient_accumulation_steps: 16
gradient_checkpointing: true
bf16: true
deepspeed: cfgs/ds_zero_1.json
dataloader_drop_last: true
# optim
optim: adamw_torch
max_grad_norm: 1.0
learning_rate: 1.0e-4
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8
weight_decay: 0.0
# lr scheduler (per-stage schedule is configured in run_progressive_train.py)
use_constant_with_warmup_decay_scheduler: true
lr_scheduler_kwargs:
  {
    "lr_decay_starting_step": 5000,
    "lr_decay_steps": 1000,
    "lr_decay_style": "1-sqrt",
    "lr_warmup_steps": 600,
    "lr_warmup_style": "linear",
    "min_decay_lr": 0,
  }

# ModelArguments
model_name_or_path: HuggingFaceTB/SmolLM-135M
partial_rope_version: 2-norm
rope_dim_for_mla: 4
uniform_start_point: 0
qk_tensor_path: utils/smollm1_135M-2_norm_rank.pth
svd_init_method: joint
low_rank: 16  # initial value, overridden by rank_schedule[0]
is_baseline: false
is_gqa2mha2mla: false

# DataArguments
is_nanoset: false
hf_dataset_name_or_path: HuggingFaceTB/smollm-corpus
hf_dataset_subset: cosmopedia-v2
sequence_length: 2048

# ProgressiveCompressionArguments
rank_schedule: [16, 12, 8]
steps_per_stage: [2000, 2000, 2000]
warmup_steps_per_stage: [200, 100, 100]
reset_optimizer: true
